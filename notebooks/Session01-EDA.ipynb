{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCL AI Society Machine Learning Tutorials\n",
    "### Session 01. Introduction to Numpy, Pandas and Matplotlib Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. Numpy\n",
    "2. Pandas\n",
    "3. Matplotlib\n",
    "4. EDA\n",
    "\n",
    "### Aim\n",
    "At the end of this session, you will be able to:\n",
    "- Understand the basics of numpy.\n",
    "- Understand the basics of pandas.\n",
    "- Understand the basics of matplotlib.\n",
    "- Perform a simple EDA (Exploratory Data Analysis) using the libraries above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EDA (Exploratory Data Analysis)\n",
    "To build and train machine learning models more efficiently, Exploratory Data Analysis, or EDA for short, should precede building the training model. This statistical approach was introduced by Professor John Tukey, also widely known for developing fast Fourier transform (FFT). \n",
    "The main goal of EDA is to analyze the data sets in order to understand their main characteristics, often with visualizations, summary tables and statistics. A thing to note is that visualization should be differentiated from EDA, as the former is mainly for the final stages of analysis and communication of results, while the latter is conducted at the beginning of the task.\n",
    "\n",
    "Remember the quote, \"Garbage In, Garbge Out!\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **basic order of EDA** theoretically is:\n",
    "   1. Find questions about the data set.\n",
    "   2. Find the answer to them on the data set using visualization, transformation and modelling.\n",
    "   3. Go deeper into the questions through the answers and find new questions.\n",
    "   4. Repeat tasks 1-3 iteratively until satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Import Data and Libraries\n",
    "The dataset we will use can be downloaded here: https://www.kaggle.com/c/nyc-taxi-trip-duration/data . Once you download your files, make sure to rename them to `nyc-taxi-train.csv` and `nyc-taxi-test.csv`.\n",
    "\n",
    "The dataset is provided by Google Cloud Platform and is based on the 2016 NYC Yellow Cab trip record. Kaggle datasets are normally well-preprocessed, so practicing EDA with a Kaggle dataset can be a good start for beginners.\n",
    "\n",
    "The libraries that we covered so far (Numpy, Pandas, and Matplotlib) are your main tools for EDA. Additonally, `seaborn` is another famous library, used mainly for visualization.\n",
    "\n",
    "#### Because the size of the data is huge, we could not upload it to github. Please download the data from the link above and place it in your 'data' directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since it takes very long time to load the original dataset\n",
    "train = pd.read_csv(\"./data/nyc-taxi-train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what the data is comprised of.\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./data/nyc-taxi-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Check the Characteristics\n",
    "- `info()` : Gives brief information\n",
    "- `shape` : Returns data shape, (rows, columns)\n",
    "- `dtypes` : Returns data types of each columns\n",
    "- `describe()` : Returns the data statistics\n",
    "- `keys()` : Returns the the keys of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Do: Brief Information of the dataset\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Do: Return shape of our data\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Do: Return data types of each field (column name)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To Do: Return the data statistics\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Check the Values\n",
    "Checking for null values, anamolies and outliers in a dataset is an essential step in EDA before you actually apply ML on it.\n",
    "\n",
    "As this dataset is well-preprocess, you won't see any null values. However, do find out and learn how to deal with missing values or null values. We have discussed this in our pandas session. It is very normal to have corrupt data in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Do: Return the keys of the columns\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()   # Remember this trick? - from the pandas session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum and maximum longitude in trainset\n",
    "min(train.pickup_longitude.min(), train.dropoff_longitude.min()), \\\n",
    "max(train.pickup_longitude.max(), train.dropoff_longitude.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum and maximum latitude in trainset\n",
    "min(train.pickup_latitude.min(), train.dropoff_latitude.min()), \\\n",
    "max(train.pickup_latitude.max(), train.dropoff_latitude.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum and maximum longitude test set\n",
    "min(test.pickup_longitude.min(), test.dropoff_longitude.min()), \\\n",
    "max(test.pickup_longitude.max(), test.dropoff_longitude.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum and maximum latitude test\n",
    "min(test.pickup_latitude.min(), test.dropoff_latitude.min()), \\\n",
    "max(test.pickup_latitude.max(), test.dropoff_latitude.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are very similar, as we expected. Next, run the code below and get a histogram of all the pickup latitudes in the training set. The plot should set its bounds to the biggest and smallest value in that dataset. Are you surprised by what you see? What can you do to get a more informative representation of this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train['pickup_latitude'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 EDA Exercise (optional)\n",
    "\n",
    "Below, assign to X the first 100 trips that have `trip_duration<10000` and to Y - the first 100 trips that have `trip_duration>10000`. Run your code and observe the histogram. What conjecture can you make based on this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here\n",
    "X = #TODO\n",
    "Y = #TODO\n",
    "\n",
    "# This will visualise the data you selected\n",
    "bins = [1, 2, 3, 4, 5, 6]\n",
    "plt.hist(X, bins, alpha=0.5)\n",
    "plt.hist(Y, bins, alpha=0.5)\n",
    "plt.xlabel(\"Number of passengers\")\n",
    "plt.ylabel(\"Number of trips\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Making Derivative Attributes\n",
    "\n",
    "ML models digest only what we feed them. You cannot expect an ML model to know that the distance from the pickup spot to the dropoff spot is important. You have to make a new attribute for the distance and feed that into your model if you want it to take this into consideration.\n",
    "\n",
    "In general, if you come up with a new informative attribute that can possibly boost the model performance, consider adding it to your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the trip distance in miles\n",
    "# based on https://stackoverflow.com/questions/27928/\n",
    "# Returns distance in miles\n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    p = 0.017453292519943295 # Pi/180\n",
    "    a = 0.5 - np.cos((lat2 - lat1) * p)/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) / 2\n",
    "    return 0.6213712 * 12742 * np.arcsin(np.sqrt(a)) # 2*R*asin..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['distance'] = distance(train.pickup_latitude, \n",
    "                             train.pickup_longitude, \n",
    "                             train.dropoff_latitude, \n",
    "                             train.dropoff_longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just the start of EDA. As mentioned above, having full insight on the data will help you build a stronger machine learning model. Find more materials and implement EDA yourself.\n",
    "\n",
    "### What do I do next?\n",
    "\n",
    "## MAKE YOUR OWN WONDERFUL EDA!\n",
    "\n",
    "### Challenge: Try to make a map using the logitude and latitude data from the taxicab data set above. Treating the logitude as X and the latitude as Y, draw a scatter plot and that should give you a good-looking map.\n",
    "\n",
    "The below websites should be helpful for your further study of EDA:\n",
    "- [Exploratory data analysis on Wikipedia](https://en.wikipedia.org/wiki/Exploratory_data_analysis)\n",
    "- [What is Exploratory Data Analysis?](https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15)\n",
    "- [Introduction to Exploratory Data Analysis in Python](https://medium.com/python-pandemonium/introduction-to-exploratory-data-analysis-in-python-8b6bcb55c190)\n",
    "- [Kaggle: New York City Taxi trip duration notebooks](https://www.kaggle.com/c/nyc-taxi-trip-duration/notebooks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
